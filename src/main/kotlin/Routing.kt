package com.pawlowski

import ai.koog.ktor.aiAgent
import ai.koog.prompt.executor.clients.deepseek.DeepSeekModels
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import io.ktor.resources.*
import io.ktor.serialization.kotlinx.json.*
import io.ktor.server.application.*
import io.ktor.server.plugins.contentnegotiation.*
import io.ktor.server.request.*
import io.ktor.server.resources.*
import io.ktor.server.response.*
import io.ktor.server.routing.*
import kotlinx.serialization.Serializable

@Serializable
data class LLMRequest(
    val prompt: String,
    val systemPrompt: String
)

@Serializable
data class LLMResponse(
    val response: String
)

fun Application.configureRouting() {
    install(Resources)
    routing {
        get("/") {
            call.respondText("HeatingAgent AI - Internal API")
        }
        
        // Wewnƒôtrzny endpoint do wywo≈Çywania LLM przez agenta (u≈ºywa Koog)
        post("/internal/llm/decision") {
            try {
                val request = call.receive<LLMRequest>()
                // Po≈ÇƒÖcz system prompt z promptem u≈ºytkownika
                val fullPrompt = "${request.systemPrompt}\n\n${request.prompt}"
                
                println("üîµ Wywo≈Çujƒô Ollama LLM przez Koog (prompt length: ${fullPrompt.length} chars)")
                
                // U≈ºyj Koog aiAgent - Koog automatycznie u≈ºyje modelu Ollama skonfigurowanego w Frameworks.kt
                // aiAgent wymaga parametru model - u≈ºywamy OpenAIModels jako placeholder (Koog wybierze Ollama z konfiguracji)
                // W rzeczywisto≈õci Koog powinien automatycznie wybraƒá Ollama gdy jest skonfigurowany
                // TODO: Znale≈∫ƒá w≈Ça≈õciwy spos√≥b na okre≈õlenie modelu Ollama w Koog
                val responseText = aiAgent(input = fullPrompt, model = ollamaModel)
                
                println("‚úÖ Ollama odpowiedzia≈Ç przez Koog (length: ${responseText.length} chars)")
                
                call.respond(LLMResponse(responseText))
            } catch (e: Exception) {
                println("‚ùå B≈ÇƒÖd w endpoincie LLM: ${e.javaClass.simpleName} - ${e.message}")
                e.printStackTrace()
                call.respond(io.ktor.http.HttpStatusCode.InternalServerError, 
                    mapOf("error" to (e.message ?: "Unknown error")))
            }
        }
        
        get<Articles> { article ->
            // Get all articles ...
            call.respond("List of articles sorted starting from ${article.sort}")
        }
    }
}

@Serializable
@Resource("/articles")
class Articles(val sort: String? = "new")
